{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonin97/bc_thesis/blob/main/substitution_cipher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VlU8g1SI0c6f",
      "metadata": {
        "id": "VlU8g1SI0c6f"
      },
      "source": [
        "### Google Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xefNKVNZTTj4",
      "metadata": {
        "id": "xefNKVNZTTj4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "my_path = '/content/drive/My Drive/bc_crypto'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6NSizevNSyBM",
      "metadata": {
        "id": "6NSizevNSyBM"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local environment"
      ],
      "metadata": {
        "id": "PxLmWtYHC_5L"
      },
      "id": "PxLmWtYHC_5L"
    },
    {
      "cell_type": "code",
      "source": [
        "my_path =  '.'"
      ],
      "metadata": {
        "id": "ic7wFvgnDCyD"
      },
      "id": "ic7wFvgnDCyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "UMLrQXyryHef",
      "metadata": {
        "id": "UMLrQXyryHef"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bfbecf3-3faf-42f4-a6f5-1a903f943dce",
      "metadata": {
        "id": "9bfbecf3-3faf-42f4-a6f5-1a903f943dce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import regex\n",
        "from unidecode import unidecode\n",
        "import math\n",
        "import csv\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import random\n",
        "import pandas as pd\n",
        "import copy\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.cm import get_cmap\n",
        "from matplotlib.lines import Line2D\n",
        "from scipy.stats import chisquare\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FvWoG_EbyLwR",
      "metadata": {
        "id": "FvWoG_EbyLwR"
      },
      "source": [
        "### Loading text data\n",
        "for testing and analysis purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EakECxK9yFED",
      "metadata": {
        "id": "EakECxK9yFED"
      },
      "outputs": [],
      "source": [
        "# simple test for helper functions testing\n",
        "format_test_text = 'Donald John Trump (born June 14, 1946) is an American politician, media personality, and businessman?! sößáěř'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kL9actOQyPIg",
      "metadata": {
        "id": "kL9actOQyPIg"
      },
      "outputs": [],
      "source": [
        "# loading the testing data from Robinson Crusoe\n",
        "with open(f'{my_path}/wikidata/testing_texts_robinson.txt', 'r') as file:\n",
        "  content = file.read().replace('\\n', '')\n",
        "\n",
        "  # array of 50 texts of length 512\n",
        "  testing_texts = [content[i:i+512] for i in range(0, 512*50, 512)]\n",
        "\n",
        "# function to retrieve testing texts from Robinson Crusoe\n",
        "def get_testing_texts(n=1):\n",
        "  if n == 1:\n",
        "    return random.sample(testing_texts, n)[0]\n",
        "  return random.sample(testing_texts, n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ACgYOOFny8Kf",
      "metadata": {
        "id": "ACgYOOFny8Kf"
      },
      "outputs": [],
      "source": [
        "# loading the whole book Peter Pan used for creating the n-gram matrix\n",
        "file_path = f'{my_path}/wikidata/peter_pan.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "  petr_pan = file.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uo2YVktH18t1",
      "metadata": {
        "id": "uo2YVktH18t1"
      },
      "outputs": [],
      "source": [
        "# for creating the figures 4 & 5\n",
        "texts_1024_10 = [testing_texts[i * 2] + testing_texts[i * 2 + 1] for i in range(10)]\n",
        "texts_128_10 = [testing_texts[i][:128] for i in range(10)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L6eNMaUPNmHb",
      "metadata": {
        "id": "L6eNMaUPNmHb"
      },
      "outputs": [],
      "source": [
        "# loading data from the wikipedia webscrapping\n",
        "file_path = f'{my_path}/wikidata/512_data_encoded.csv'\n",
        "\n",
        "# first column = string, encrypted text | second  column = float, level of encoding <0, 1>\n",
        "column_defaults = [tf.string, tf.float32]\n",
        "\n",
        "# create a CSV dataset\n",
        "dataset = tf.data.experimental.CsvDataset(\n",
        "  file_path,\n",
        "  record_defaults=column_defaults,\n",
        "  header=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-ZiCriJrUctw",
      "metadata": {
        "id": "-ZiCriJrUctw"
      },
      "source": [
        "### RNN import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ujoTToI0WiKw",
      "metadata": {
        "id": "ujoTToI0WiKw"
      },
      "outputs": [],
      "source": [
        "# custom n-gram split function for the model\n",
        "def ngram_split(text):\n",
        "  characters = tf.strings.unicode_split(text, 'UTF-8')\n",
        "  # Create character n-grams\n",
        "  return tf.strings.ngrams(\n",
        "    characters,\n",
        "    ngram_width=ngram_size, # global parameter\n",
        "    separator=''  # join n-grams without spaces\n",
        "  )\n",
        "ngram_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P5hv533QUfcL",
      "metadata": {
        "id": "P5hv533QUfcL"
      },
      "outputs": [],
      "source": [
        "# model to be loaded (1-10)\n",
        "model_nr = '10'\n",
        "\n",
        "model_path = f'{my_path}/models/model_{model_nr}.tf'\n",
        "\n",
        "# loading the model\n",
        "rnn_model = tf.keras.models.load_model(model_path, custom_objects={\n",
        "  'ngram_split': ngram_split\n",
        "  })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QS0l3NepY0Bx",
      "metadata": {
        "id": "QS0l3NepY0Bx"
      },
      "outputs": [],
      "source": [
        "# defining the RNN function\n",
        "def rnn_estimate_function(text):\n",
        "  # turning the string into tf dataset\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(([text]))\n",
        "  dataset = dataset.batch(1)\n",
        "  rnn_estimate = rnn_model.predict(dataset, verbose=0)[0][0]\n",
        "  return(rnn_estimate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ffff869-d91f-4dcc-8690-4086f5bd605b",
      "metadata": {
        "id": "2ffff869-d91f-4dcc-8690-4086f5bd605b"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8df5d23-ccdc-495a-a514-040d23de58e8",
      "metadata": {
        "id": "a8df5d23-ccdc-495a-a514-040d23de58e8"
      },
      "outputs": [],
      "source": [
        "# formatting the text to contain only 27 characters A-Z and _ as whitespace\n",
        "def format_text(input_text):\n",
        "\n",
        "  # if text already formatted\n",
        "  if regex.match(r'^[A-Z_]+$', input_text):\n",
        "    return input_text\n",
        "\n",
        "  # cleaning the text from all language-specific characters\n",
        "  cleaned_text = unidecode(input_text)\n",
        "\n",
        "  # remove non-alpha characters and transform the text into words connected by an underscore\n",
        "  words = ''.join(char if char.isalpha() or char.isspace() else '' for char in cleaned_text).split()\n",
        "  formatted_text = '_'.join(word.upper() for word in words)\n",
        "\n",
        "  # sanity check\n",
        "  assert regex.match(r'^[A-Z_]+$', formatted_text), 'contains forbidden characters'\n",
        "\n",
        "  return formatted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6908b078-a45e-4ca7-b378-99af0191e855",
      "metadata": {
        "id": "6908b078-a45e-4ca7-b378-99af0191e855"
      },
      "outputs": [],
      "source": [
        "# formatting + vectorizing A-Z_ string --> (0-26) vector\n",
        "def text_to_vector(text):\n",
        "  text = format_text(text)\n",
        "  # assigning ordinate (26 for the whitespace, underscore)\n",
        "  char_to_number = {char: ord(char) - ord('A') if char.isalpha() else 26 for char in text}\n",
        "  vector = np.array([char_to_number[char] for char in text])\n",
        "  return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9a1598-dc6c-4091-8d71-b78ec42418e2",
      "metadata": {
        "id": "cc9a1598-dc6c-4091-8d71-b78ec42418e2"
      },
      "outputs": [],
      "source": [
        "# (0-26) vector --> A-Z_ string\n",
        "def vector_to_text(vector):\n",
        "  number_to_char = {i: chr(i + ord('A')) if i < 26 else '_' for i in range(27)}\n",
        "  text = ''.join(number_to_char[number] for number in vector)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33b0a39-cf9e-4dfb-9c81-7f3bdeac1ba8",
      "metadata": {
        "id": "c33b0a39-cf9e-4dfb-9c81-7f3bdeac1ba8"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "print(vector_to_text(text_to_vector(format_test_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b9401a-ce04-492f-8830-66add2817384",
      "metadata": {
        "id": "35b9401a-ce04-492f-8830-66add2817384"
      },
      "source": [
        "### Substitute cipher functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18de4260-5d72-4bd6-818a-a335fca6644d",
      "metadata": {
        "id": "18de4260-5d72-4bd6-818a-a335fca6644d"
      },
      "outputs": [],
      "source": [
        "# key is a permutation vector of numbers 0-26 (A-Z+_)\n",
        "# plaintext can be inserted without formatting, will be formatted during text_to_vector call\n",
        "def substitute_encrypt(plaintext, key):\n",
        "  plaintext_vector = text_to_vector(plaintext)\n",
        "  encrypted_vector = np.array([key[plaintext_vector[i]] for i in range(len(plaintext_vector))])\n",
        "  encrypted_text = vector_to_text(encrypted_vector)\n",
        "  return encrypted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a8131f-ce98-4fe7-bc1c-5d57730837ff",
      "metadata": {
        "id": "c2a8131f-ce98-4fe7-bc1c-5d57730837ff"
      },
      "outputs": [],
      "source": [
        "# same as encrypting, but creating an inversion of the key beforehand\n",
        "def substitute_decrypt(ciphertext, key):\n",
        "  inverse_key = np.array([np.where(key == i)[0][0] for i in range(27)])\n",
        "  encrypted_vector = text_to_vector(ciphertext)\n",
        "  plaintext_vector = np.array([inverse_key[encrypted_vector[i]] for i in range(len(encrypted_vector))])\n",
        "  decrypted_text = vector_to_text(plaintext_vector)\n",
        "  return decrypted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae31440-32f8-42ed-8922-d1bb9ebff00b",
      "metadata": {
        "id": "eae31440-32f8-42ed-8922-d1bb9ebff00b"
      },
      "outputs": [],
      "source": [
        "# creating a permutation key, that will always yield the same percentage of vector elements to be switched up\n",
        "# percent_mix - how many percent of the vector elements should be switched up\n",
        "# n_elements - how many elements should the vector contain (0 - n-1)\n",
        "def create_permutation_key(n_displace, n_elements=27):\n",
        "    assert n_displace <= n_elements\n",
        "    array = np.arange(n_elements)\n",
        "    # ensuring no bug\n",
        "    if n_displace == 1: n_displace = 0\n",
        "    # randomly choose indices to displace\n",
        "    indices = np.arange(n_elements)\n",
        "    np.random.shuffle(indices)\n",
        "    indices_to_displace = sorted(indices[:n_displace])\n",
        "\n",
        "    # creating a copy to keep track of indices that can be still used\n",
        "    indices_remaining = [i for i in indices_to_displace]\n",
        "\n",
        "    for index in indices_to_displace:\n",
        "        # flag for adding an element back to remaining\n",
        "        add = False\n",
        "\n",
        "        # if index hasn't been used for swap yet, it must be removed from the options\n",
        "        if index in indices_remaining:\n",
        "            indices_remaining.remove(index) # removing current index to ensure displacement\n",
        "            add = True # flag for adding it back again after iteration\n",
        "\n",
        "            # edge case - no more elements to swap the index with\n",
        "            if len(indices_remaining) == 0:\n",
        "                indices_to_displace.remove(index)\n",
        "                # randomly swapping the last element with one of the others - certainly it won't happen, that displacement wouldn't hapen\n",
        "                swap_index = np.random.choice(indices_to_displace)\n",
        "                array[index] = array[swap_index]\n",
        "                array[swap_index] = index\n",
        "                break\n",
        "\n",
        "        new_value = np.random.choice(indices_remaining) # choosing a value to insert to the given index\n",
        "        array[index] = new_value # swapping the elements\n",
        "        indices_remaining.remove(new_value) # removing the used index (can't be used twice)\n",
        "        if add:\n",
        "            indices_remaining.append(index) # adding back the index as value\n",
        "\n",
        "    if type(array) == list:\n",
        "      return array\n",
        "    return array.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bCSeqYtFIdZj",
      "metadata": {
        "id": "bCSeqYtFIdZj"
      },
      "outputs": [],
      "source": [
        "# function that takes in a key and performs exactly n_change changes in it\n",
        "def change_permutation_key(n_change, key):\n",
        "  key = list(key)\n",
        "  indices = np.arange(len(key))\n",
        "  np.random.shuffle(indices)\n",
        "  indices_to_displace = sorted(indices[:n_change])\n",
        "  for i in range(len(indices_to_displace) - 1):\n",
        "    key[indices_to_displace[i % n_change]], key[indices_to_displace[(i + 1) % n_change]] = key[indices_to_displace[(i + 1) % n_change]], key[indices_to_displace[i % n_change]]\n",
        "  return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aVAEJwvIaiMO",
      "metadata": {
        "id": "aVAEJwvIaiMO"
      },
      "outputs": [],
      "source": [
        "# comparing two texts of the same length, returning percentage of same placed characters\n",
        "def compare_texts(text1, text2):\n",
        "  assert len(text1) == len(text2)\n",
        "  return sum([text1[i] == text2[i] for i in range(len(text1))]) / len(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lHk2D77WPvEV",
      "metadata": {
        "id": "lHk2D77WPvEV"
      },
      "outputs": [],
      "source": [
        "# local search for a key to substitution cipher\n",
        "# ciphertext - encrypted using unknown key\n",
        "# n_iterations - iterations of the Monte Carlo algorithm\n",
        "# likelihood_func - either markov chains analysis likelihood or the RNN estimate\n",
        "# original_text - the original plaintext, for analysis purposes\n",
        "# key - if a predefined key should be used, else a random key\n",
        "# return_key - if the final key should be returned, else analysis information is returned\n",
        "def monte_carlo_algorithm(ciphertext, n_iterations, likelihood_func, original_text=None, key=None, return_key=False):\n",
        "\n",
        "  # array for storing algorithm data\n",
        "  confusion_matrix_data = []\n",
        "  likelihoods = []\n",
        "  true_percentages = []\n",
        "\n",
        "  # creating a random key - vector of numbers 0 - 26\n",
        "  if not np.any(key):\n",
        "    key = np.random.permutation(27)\n",
        "  # using key passed as a parameter (e.g. fixed points of permutation simulation)\n",
        "  else:\n",
        "    key = np.array(key)\n",
        "\n",
        "  # initial evaluation values\n",
        "  iter_decrypt = substitute_decrypt(ciphertext, key)\n",
        "  iter_likelihood = likelihood_func(iter_decrypt)\n",
        "  likelihoods.append(iter_likelihood)\n",
        "\n",
        "  # if plaintext is passed as an argument, we can observe the true percentage\n",
        "  if original_text:\n",
        "    true_percentage = compare_texts(iter_decrypt, original_text)\n",
        "    true_percentages.append(true_percentage)\n",
        "\n",
        "  # tracking made actions\n",
        "  swaps, rejected, randomwalk = 0,0,0\n",
        "\n",
        "  for _ in range(n_iterations):\n",
        "    new_key = np.copy(key)\n",
        "\n",
        "    # randomly swapping two elements of the key vector\n",
        "    index1, index2 = np.random.choice(len(new_key), size=2, replace=False)\n",
        "    new_key[index1], new_key[index2] = new_key[index2], new_key[index1]\n",
        "\n",
        "    # decrypting text with the new vector and computing its likelihood\n",
        "    new_iter_decrypt = substitute_decrypt(ciphertext, new_key)\n",
        "    new_iter_likelihood = likelihood_func(new_iter_decrypt)\n",
        "\n",
        "    if original_text:\n",
        "      new_true_percentage = compare_texts(new_iter_decrypt, original_text)\n",
        "      # boolean --> has the true likelihood actually improved?\n",
        "      true_q = new_true_percentage > true_percentage\n",
        "\n",
        "    # computing ratio --> how much has the internal likelihood improved?\n",
        "    q = new_iter_likelihood / iter_likelihood\n",
        "\n",
        "    # if the internal likelihood improved, swap is approved --> hill climb\n",
        "    if (q > 1):\n",
        "      key = new_key\n",
        "      iter_decrypt = new_iter_decrypt\n",
        "      iter_likelihood = new_iter_likelihood\n",
        "      swaps += 1\n",
        "\n",
        "      if original_text:\n",
        "        true_percentage = new_true_percentage\n",
        "        # computing data for confussion matrix\n",
        "        # 3 == True Positive\n",
        "        if true_q:\n",
        "          confusion_matrix_data.append(3)\n",
        "        # 1 == False Positive\n",
        "        else:\n",
        "          confusion_matrix_data.append(1)\n",
        "\n",
        "    # if the internal likelihood has only slightly decreased + odds --> random walk\n",
        "    elif (q > 0.9 and np.random.rand() < 0.001):\n",
        "      key = new_key\n",
        "      iter_decrypt = new_iter_decrypt\n",
        "      iter_likelihood = new_iter_likelihood\n",
        "      randomwalk += 1\n",
        "\n",
        "      if original_text:\n",
        "        true_percentage = new_true_percentage\n",
        "        # computing data for confussion matrix\n",
        "        # 3 == True Positive\n",
        "        if true_q:\n",
        "          confusion_matrix_data.append(3)\n",
        "        # 1 == False Positive\n",
        "        else:\n",
        "          confusion_matrix_data.append(1)\n",
        "\n",
        "    # if the internal likelihood hasn't improved --> reject\n",
        "    else:\n",
        "      rejected += 1\n",
        "\n",
        "    if original_text:\n",
        "      # computing data for confussion matrix\n",
        "      # 2 == True Negative\n",
        "      if not true_q:\n",
        "        confusion_matrix_data.append(2)\n",
        "      # 0 == False Negative\n",
        "      else:\n",
        "        confusion_matrix_data.append(0)\n",
        "\n",
        "    likelihoods.append(iter_likelihood)\n",
        "    if original_text:\n",
        "      true_percentages.append(true_percentage)\n",
        "\n",
        "    # console log\n",
        "    if (_ % 1000 == 0):\n",
        "        print(f'{_}. iteration', iter_likelihood)\n",
        "\n",
        "  print(f'swaps/rejected/random: {swaps}/{rejected}/{randomwalk}')\n",
        "  if return_key:\n",
        "    return key\n",
        "  return likelihoods, true_percentages, confusion_matrix_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EUVlaUylb9v2",
      "metadata": {
        "id": "EUVlaUylb9v2"
      },
      "source": [
        "### Breaking Caesar's cipher (Figure 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeHsG2N9g6Hb",
      "metadata": {
        "id": "aeHsG2N9g6Hb"
      },
      "outputs": [],
      "source": [
        "# obtained from the web Letter Frequencies in the English Language\n",
        "alphabet_freq = {\n",
        "  'E' :\t0.111607,\n",
        "  'M' :\t0.030129,\n",
        "  'A' :\t0.084966,\n",
        "  'H' :\t0.030034,\n",
        "  'R' :\t0.075809,\n",
        "  'G' :\t0.024705,\n",
        "  'I' :\t0.075448,\n",
        "  'B' :\t0.020720,\n",
        "  'O' :\t0.071635,\n",
        "  'F' :\t0.018121,\n",
        "  'T' :\t0.069509,\n",
        "  'Y' :\t0.017779,\n",
        "  'N' :\t0.066544,\n",
        "  'W' :\t0.012898,\n",
        "  'S' :\t0.057351,\n",
        "  'K' :\t0.011016,\n",
        "  'L' :\t0.054893,\n",
        "  'V' :\t0.010074,\n",
        "  'C' :\t0.045388,\n",
        "  'X' :\t0.002902,\n",
        "  'U' :\t0.036308,\n",
        "  'Z' :\t0.002722,\n",
        "  'D' :\t0.033844,\n",
        "  'J' :\t0.001965,\n",
        "  'P' :\t0.031671,\n",
        "  'Q' :\t0.001962,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xwkUKvdtVv-x",
      "metadata": {
        "id": "xwkUKvdtVv-x"
      },
      "outputs": [],
      "source": [
        "# Apply Caesar cipher to the text with a given shift\n",
        "# expects texts with only A-Z letters, no whitespaces\n",
        "def caesar_shift(text, shift):\n",
        "  shifted_text = ''\n",
        "  for char in text.upper():\n",
        "    if 'A' <= char <= 'Z':\n",
        "      shifted_text += chr((ord(char) - ord('A') + shift) % 26 + ord('A'))\n",
        "    else:\n",
        "      shifted_text += char\n",
        "  return shifted_text\n",
        "\n",
        "# calculates frequencies for all letters in the alphabet for a given text\n",
        "def letter_frequency(text, alphabet_frequency):\n",
        "    text = text.upper()  # normalize the input to uppercase\n",
        "    letter_counts = {chr(i): 0 for i in range(ord('A'), ord('Z') + 1)}\n",
        "    total_letters = 0\n",
        "    for char in text:\n",
        "      if 'A' <= char <= 'Z':\n",
        "        letter_counts[char] += 1\n",
        "        total_letters += 1\n",
        "\n",
        "    # calculate frequencies and store into the dictionary\n",
        "    if total_letters > 0:\n",
        "      return {letter: count / total_letters for letter, count in letter_counts.items()}\n",
        "    else:\n",
        "      return {letter: 0 for letter in letter_counts}\n",
        "\n",
        "# plot function for the Figure 2\n",
        "def plot_letter_frequencies(text, alphabet_frequency):\n",
        "  plt.figure(figsize=(15, 30))  # Adjusted figure size for 3x9 grid\n",
        "  letters = sorted(alphabet_frequency.keys())\n",
        "\n",
        "  axs = []  # list to store axes to access later for title update\n",
        "  chi2_values = []  # store chi-squared statistics for identifying the minimal value\n",
        "\n",
        "  # generate plots for all 26 shifts\n",
        "  for shift in range(26):\n",
        "    shifted_text = caesar_shift(text, shift)\n",
        "    letter_freqs = letter_frequency(shifted_text, alphabet_frequency)\n",
        "\n",
        "    # calculate Chi-Squared test\n",
        "    observed = [letter_freqs.get(letter, 0) * len(shifted_text) for letter in letters]\n",
        "    expected = [alphabet_frequency[letter] * len(shifted_text) for letter in letters]\n",
        "    chi2_stat, p_val = chisquare(f_obs=observed, f_exp=expected)\n",
        "    chi2_values.append(chi2_stat)\n",
        "\n",
        "    # create a subplot for this shift in a 3x9 grid\n",
        "    ax = plt.subplot(9, 3, shift + 1)\n",
        "    axs.append(ax)\n",
        "\n",
        "    alphabet_freqs = [alphabet_frequency[letter] for letter in letters]\n",
        "    text_freqs = [letter_freqs.get(letter, 0) for letter in letters]\n",
        "\n",
        "    # plot alphabet frequencies as bars and text frequencies as a line\n",
        "    ax.bar(letters, alphabet_freqs, color='blue', alpha=0.7)\n",
        "    ax.plot(letters, text_freqs, 'r--')\n",
        "    ax.set_title(f'Shift {shift} | χ² = {chi2_stat:.3f}')\n",
        "    ax.set_ylim(0, max(alphabet_freqs) + 0.05)  # adjust ylim to better fit the titles\n",
        "\n",
        "  # identify the subplot with minimal Chi-Squared value\n",
        "  min_chi2_index = chi2_values.index(min(chi2_values))\n",
        "  axs[min_chi2_index].set_title(f'Shift {min_chi2_index} | χ² = {chi2_values[min_chi2_index]:.3f}', color='red')\n",
        "\n",
        "  # use the last subplot to place the legend\n",
        "  ax = plt.subplot(9, 3, 27)\n",
        "  ax.legend(handles=[plt.Rectangle((0,0),1,1, color='blue', alpha=0.7), plt.Line2D([0],[0], color='red', linestyle='--')],\n",
        "            labels=['English letters Frequency', 'Text letters Frequency'], loc='center')\n",
        "  ax.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(f'{my_path}/plots/caesar.png')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oXjtjSL-Ll6s",
      "metadata": {
        "id": "oXjtjSL-Ll6s"
      },
      "source": [
        "#### Plot used in Figure 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fKAyglExhAQ9",
      "metadata": {
        "id": "fKAyglExhAQ9"
      },
      "outputs": [],
      "source": [
        "text_example = 'The only thing that interferes with my learning is my education'\n",
        "shift_key = tuple([i for i in range(6,27)]+[i for i in range(0,6)])\n",
        "text_example = substitute_encrypt(text_example, shift_key)\n",
        "plot_letter_frequencies(text_example, alphabet_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9406e2b-33d6-4635-9b1b-e9043e8a9a9b",
      "metadata": {
        "id": "f9406e2b-33d6-4635-9b1b-e9043e8a9a9b"
      },
      "source": [
        "### Markov Chains transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17db233-1013-4bb3-82c1-96ec0a0d98a4",
      "metadata": {
        "id": "b17db233-1013-4bb3-82c1-96ec0a0d98a4"
      },
      "outputs": [],
      "source": [
        "# calculating digram matrix based on text\n",
        "# only for alphabet of length 27 (A-Z + _)\n",
        "def digram_matrix(text, logsafe=False):\n",
        "  # logsafe - robust for the calculation (language analysis)\n",
        "  if logsafe:\n",
        "    matrix = np.ones((27, 27))\n",
        "  # not logsafe - more precise\n",
        "  else:\n",
        "    matrix = np.zeros((27, 27))\n",
        "  i = 0\n",
        "  while i + 1 < len(text):\n",
        "    ith = 26 if text[i] == '_' else (ord(text[i]) - ord('A'))\n",
        "    jth = 26 if text[i + 1] == '_' else (ord(text[i + 1]) - ord('A'))\n",
        "    matrix[ith][jth] += 1\n",
        "    i += 1\n",
        "  return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee06296-815f-4c74-b402-b0dd20f92474",
      "metadata": {
        "id": "6ee06296-815f-4c74-b402-b0dd20f92474"
      },
      "outputs": [],
      "source": [
        "# creating digram transition matrix for the English language\n",
        "# based on Petr Pan book\n",
        "petr_pan = format_text(petr_pan)\n",
        "TM_en = digram_matrix(petr_pan, logsafe=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac76be9-9a79-4de4-b3bc-53ea2008c7f8",
      "metadata": {
        "id": "dac76be9-9a79-4de4-b3bc-53ea2008c7f8"
      },
      "outputs": [],
      "source": [
        "# calculating likelihood from digram matrix of English language and digram matrix of the ciphertext\n",
        "def markov_chain_analysis(text):\n",
        "  likelihood = np.sum(np.log(TM_en) * digram_matrix(text))\n",
        "  return likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IusDdvSLzob4",
      "metadata": {
        "id": "IusDdvSLzob4"
      },
      "source": [
        "### Comparing the hill climb with the Monte Carlo algorithm (Figure 4, 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q6tJCasf5Sf4",
      "metadata": {
        "id": "Q6tJCasf5Sf4"
      },
      "outputs": [],
      "source": [
        "# local search for a key to substitution cipher - hill climb method\n",
        "# ciphertext - encrypted using unknown key\n",
        "# n_iterations - iterations of the hill climb algorithm\n",
        "# likelihood_func - either markov chains analysis likelihood or the RNN estimate\n",
        "def hill_climb_plaus(ciphertext, n_iterations, likelihood_func, hill=True):\n",
        "  # np.arange to ensure the initial key is a derangement of the true key (worst case scenario)\n",
        "  key = np.arange(27)\n",
        "\n",
        "\n",
        "  iter_decrypt = substitute_decrypt(ciphertext, key)\n",
        "  # initial likelihood\n",
        "  iter_likelihood = likelihood_func(iter_decrypt)\n",
        "  likelihoods = [iter_likelihood]\n",
        "\n",
        "  if hill:\n",
        "    for _ in range(n_iterations):\n",
        "      top_swap = (0, 0)\n",
        "      top_plaus = likelihoods[-1]\n",
        "      # 27 * 26 / 2 = 351 běží loop\n",
        "      for index1 in range(0, len(key)):\n",
        "        for index2 in range(index1 + 1, len(key)):\n",
        "          new_key = np.copy(key)\n",
        "          new_key[index1], new_key[index2] = new_key[index2], new_key[index1]\n",
        "          new_iter_decrypt = substitute_decrypt(ciphertext, new_key)\n",
        "          new_iter_likelihood = likelihood_func(new_iter_decrypt)\n",
        "          if new_iter_likelihood > top_plaus:\n",
        "            top_plaus = new_iter_likelihood\n",
        "            top_swap = (index1, index2)\n",
        "      # if no change was done\n",
        "      if top_plaus == likelihoods[-1]:\n",
        "        likelihoods = likelihoods + [likelihoods[-1] for i in range(n_iterations - len(likelihoods))]\n",
        "        return likelihoods\n",
        "\n",
        "      key[top_swap[0]], key[top_swap[1]] = key[top_swap[1]], key[top_swap[0]]\n",
        "\n",
        "      likelihoods.append(top_plaus)\n",
        "    return likelihoods\n",
        "\n",
        "\n",
        "  else:\n",
        "    for _ in range(n_iterations * 351):\n",
        "      new_key = np.copy(key)\n",
        "\n",
        "      # randomly swapping two elements of the key vector\n",
        "      index1, index2 = np.random.choice(len(new_key), size=2, replace=False)\n",
        "      new_key[index1], new_key[index2] = new_key[index2], new_key[index1]\n",
        "\n",
        "      # decrypting text with the new vector and computing its likelihood\n",
        "      new_iter_decrypt = substitute_decrypt(ciphertext, new_key)\n",
        "      new_iter_likelihood = likelihood_func(new_iter_decrypt)\n",
        "\n",
        "      q = new_iter_likelihood / iter_likelihood\n",
        "\n",
        "      if (q > 1):\n",
        "        key = new_key\n",
        "        iter_decrypt = new_iter_decrypt\n",
        "        iter_likelihood = new_iter_likelihood\n",
        "      elif (q > 0.95 and np.random.rand() < 0.001):\n",
        "        key = new_key\n",
        "        iter_decrypt = new_iter_decrypt\n",
        "        iter_likelihood = new_iter_likelihood\n",
        "\n",
        "      likelihoods.append(iter_likelihood)\n",
        "    return likelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9DI1aQ1T3Zsk",
      "metadata": {
        "id": "9DI1aQ1T3Zsk"
      },
      "outputs": [],
      "source": [
        "# Figure 4\n",
        "text = random.sample(texts_1024_10, 1)[0]\n",
        "\n",
        "# Figure 5\n",
        "#text = random.sample(texts_128_10, 1)[0]\n",
        "\n",
        "plaintext_likelihood = markov_chain_analysis(text)\n",
        "\n",
        "likelihoods_hillclimb = []\n",
        "# number of different initial keys\n",
        "for _ in range(5):\n",
        "  # derangement key for the decryption\n",
        "  key = create_permutation_key(27, 27)\n",
        "  ciphertext = substitute_encrypt(format_text(text), key)\n",
        "  likelihoods_hillclimb.append(hill_climb_plaus(ciphertext, 33, markov_chain_analysis))\n",
        "\n",
        "likelihoods_monte_carlo = []\n",
        "# number of different initial keys\n",
        "for _ in range(5):\n",
        "  # derangement key for the decryption\n",
        "  key = create_permutation_key(27, 27)\n",
        "  ciphertext = substitute_encrypt(format_text(text), key)\n",
        "  likelihoods_monte_carlo.append(hill_climb_plaus(ciphertext, 33, markov_chain_analysis, hill=False))\n",
        "\n",
        "# multiplying by the number of swaps\n",
        "likelihoods_hillclimb_final = []\n",
        "for i in likelihoods_hillclimb:\n",
        "  x = []\n",
        "  for j in i:\n",
        "    for _ in range(351):\n",
        "      x.append(j)\n",
        "  likelihoods_hillclimb_final.append(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OZNZ1GsX_6Iy",
      "metadata": {
        "id": "OZNZ1GsX_6Iy"
      },
      "source": [
        "#### Plot used in Figures 4 and 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pmnrlD6EAffl",
      "metadata": {
        "id": "pmnrlD6EAffl"
      },
      "outputs": [],
      "source": [
        "# get colormaps for green and blue shades\n",
        "green_cmap = get_cmap('Greens')\n",
        "blue_cmap = get_cmap('Blues')\n",
        "\n",
        "# determine the number of lines to plot\n",
        "num_hillclimb = len(likelihoods_hillclimb_final)\n",
        "num_monte_carlo = len(likelihoods_monte_carlo)\n",
        "\n",
        "# generate color shades from 40% to 100% intensity\n",
        "green_colors = [green_cmap(0.4 + 0.6 * i / (num_hillclimb - 1)) for i in range(num_hillclimb)]\n",
        "blue_colors = [blue_cmap(0.4 + 0.6 * i / (num_monte_carlo - 1)) for i in range(num_monte_carlo)]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# plot hill climb data with shades of green\n",
        "for i, color in zip(likelihoods_hillclimb_final, green_colors):\n",
        "    plt.plot(i, linestyle='-', color=color)\n",
        "\n",
        "# plot monte carlo data with shades of blue\n",
        "for j, color in zip(likelihoods_monte_carlo, blue_colors):\n",
        "\n",
        "    plt.plot(j, linestyle='-', color=color)\n",
        "\n",
        "# plot plaintext likelihood line\n",
        "plt.plot([plaintext_likelihood for _ in range(33 * 351)], linestyle='-', color='red')\n",
        "\n",
        "# create custom legend\n",
        "custom_lines = [Line2D([0], [0], color=green_cmap(0.7), lw=2),\n",
        "                Line2D([0], [0], color=blue_cmap(0.7), lw=2),\n",
        "                Line2D([0], [0], color='red', lw=2)]\n",
        "\n",
        "plt.legend(custom_lines, ['Hill Climb method', 'Monte Carlo method', 'Plaintext log-likelihood'])\n",
        "\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('log-likelihood')\n",
        "plt.grid(True)\n",
        "plt.savefig(f'{my_path}/plots/likelihoods_montecarlo_hillclimb.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VEB9HYhYT-_8",
      "metadata": {
        "id": "VEB9HYhYT-_8"
      },
      "source": [
        "### Percentage of correctly placed characters (Table 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot used in Table 3"
      ],
      "metadata": {
        "id": "fKZ03w4urT3R"
      },
      "id": "fKZ03w4urT3R"
    },
    {
      "cell_type": "code",
      "source": [
        "walt_disney_text = 'THEWAYTOGETSTARTEDISTOQUITTALKINGANDBEGINDOING'\n",
        "for i in [0, 5, 10, 15, 20, 26]:\n",
        "  key = create_permutation_key(i, 26)\n",
        "  print(substitute_encrypt(walt_disney_text, key), round(1 - (i / 26), 2) )"
      ],
      "metadata": {
        "id": "SWQrXssqrRqc"
      },
      "id": "SWQrXssqrRqc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "db7457eb-616a-400d-a8f8-4699111debd1",
      "metadata": {
        "id": "db7457eb-616a-400d-a8f8-4699111debd1"
      },
      "source": [
        "### Comparison of % of correctly placed characters and models estimates (Figure 14, 15, 18, 21, 22, 23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sty6UsGirlp1",
      "metadata": {
        "id": "sty6UsGirlp1"
      },
      "outputs": [],
      "source": [
        "total_items = 250000 # hardcoded\n",
        "text_length = 512\n",
        "\n",
        "full_dataset = dataset.shuffle(buffer_size=total_items) # to ensure uniform distribution, we need the buffer to consist of all the data\n",
        "\n",
        "# filtering only plaintext data - no encryption\n",
        "def filter_plaintext(data, label):\n",
        "    return tf.equal(label, 1.0)\n",
        "\n",
        "# Create plaintext_dataset by filtering full_dataset\n",
        "plaintext_dataset = full_dataset.filter(filter_plaintext).batch(50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oZBygaVDmQ9p",
      "metadata": {
        "id": "oZBygaVDmQ9p"
      },
      "outputs": [],
      "source": [
        "true = []\n",
        "predicted = []\n",
        "\n",
        "# for each magnitude of derangement (27) one batch (50) is tested\n",
        "for i in range(1, 28):\n",
        "  for text, _ in plaintext_dataset.take(1):\n",
        "    for datapoint in text:\n",
        "      datapoint = datapoint.numpy().decode('utf-8')\n",
        "      key = create_permutation_key(i, n_elements=27)\n",
        "      ciphertext = substitute_encrypt(datapoint, key)\n",
        "      true.append(compare_texts(datapoint, ciphertext))\n",
        "      predicted.append(rnn_estimate_function(ciphertext))\n",
        "\n",
        "sorted_true, sorted_predicted = zip(*sorted(zip(true, predicted), reverse=True))\n",
        "sorted_true = list(sorted_true)\n",
        "sorted_predicted = list(sorted_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ioflVlHhO99W",
      "metadata": {
        "id": "ioflVlHhO99W"
      },
      "source": [
        "#### Plot used in Figures 14, 15, 18, 21, 22, 23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YoU6G0b8Qw1r",
      "metadata": {
        "id": "YoU6G0b8Qw1r"
      },
      "outputs": [],
      "source": [
        "plt.scatter(range(len(sorted_predicted)), sorted_predicted, color='blue', label='Predicted', alpha=0.3)\n",
        "\n",
        "# create a line plot for the true data\n",
        "plt.plot(range(len(sorted_true)), sorted_true, color='red', label='True')\n",
        "\n",
        "# customize the plot\n",
        "plt.ylabel('likelihood')\n",
        "plt.xlabel('Index')\n",
        "plt.title('True vs Predicted likelihood')\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.savefig(f'{my_path}/plots/likelihoods_{model_nr}.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qEyD_lfEP7Dp",
      "metadata": {
        "id": "qEyD_lfEP7Dp"
      },
      "source": [
        "### Histogram RNN estimates of derangements (Figure 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vxLwRLLqwL07",
      "metadata": {
        "id": "vxLwRLLqwL07"
      },
      "outputs": [],
      "source": [
        "# true & predicted values\n",
        "true = []\n",
        "guessed = []\n",
        "for i in range(40):\n",
        "  for text, _ in plaintext_dataset.take(1):\n",
        "    for datapoint in text:\n",
        "      datapoint = datapoint.numpy().decode('utf-8')\n",
        "      key = create_permutation_key(27, n_elements=27)\n",
        "      ciphertext = substitute_encrypt(datapoint, key)\n",
        "      true.append(compare_texts(datapoint, ciphertext))\n",
        "      guessed.append(rnn_estimate_function(ciphertext))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UfQgFHFyJgHI",
      "metadata": {
        "id": "UfQgFHFyJgHI"
      },
      "source": [
        "#### Plot used in Figure 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06H00yy6xxQd",
      "metadata": {
        "id": "06H00yy6xxQd"
      },
      "outputs": [],
      "source": [
        "plt.hist(guessed, bins=10, range=(0, 1), edgecolor='black')\n",
        "\n",
        "# adding title and labels\n",
        "plt.title('Histogram of Data')\n",
        "plt.xlabel('likelihood')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# display the histogram\n",
        "plt.savefig(f'{my_path}/plots/likelihoods_histogram.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tf0omxTFSiN2",
      "metadata": {
        "id": "Tf0omxTFSiN2"
      },
      "source": [
        "### Monte Carlo algorithm results with RNN estimates (Table 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Used in Table 5"
      ],
      "metadata": {
        "id": "kfWnBUncvqUK"
      },
      "id": "kfWnBUncvqUK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q-He0rk11zkY",
      "metadata": {
        "id": "Q-He0rk11zkY"
      },
      "outputs": [],
      "source": [
        "text = 'IN_THE_HEART_OF_THE_BUSTLING_CITY_WHERE_SKYSCRAPERS_REACH_FOR_THE_CLOUDS_A_HIDDEN_OASIS_BLOOMS_A_QUAINT_PARK_ADORNED_WITH_VIBRAN'\n",
        "for i in range(10):\n",
        "  key = create_permutation_key(27, 27)\n",
        "  ciphertext = substitute_encrypt(text, key)\n",
        "  found_key = monte_carlo_algorithm(ciphertext, 2001, rnn_estimate_function, return_key=True)\n",
        "  decrypted = substitute_decrypt(ciphertext, found_key)\n",
        "  print(decrypted[:50] + '...')\n",
        "  print('likelihood: ', rnn_estimate_function(decrypted))\n",
        "  correct = sum([1 if decrypted[i] == text[i] else 0 for i in range(len(ciphertext))]) / len(ciphertext)\n",
        "  print('Correct %:', correct)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare true percentage of characters and the RNN estimate (Figure 17, 19)"
      ],
      "metadata": {
        "id": "9tsXwzONxsnS"
      },
      "id": "9tsXwzONxsnS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot used in Figures 17 and 19"
      ],
      "metadata": {
        "id": "ue3gUishv1mX"
      },
      "id": "ue3gUishv1mX"
    },
    {
      "cell_type": "code",
      "source": [
        "text = get_testing_texts(1)\n",
        "key = create_permutation_key(27, 27)\n",
        "ciphertext = substitute_encrypt(text, key)\n",
        "\n",
        "likelihoods, true_percentages, _ = monte_carlo_algorithm(ciphertext, 3000, rnn_estimate_function, original_text=text)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(likelihoods, label='RNN estimate', color='blue')\n",
        "plt.plot(true_percentages, label='% correct characters', color='orange')\n",
        "\n",
        "# add labels and title\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('percentage')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gzjxU0qnyTbj"
      },
      "id": "gzjxU0qnyTbj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSE vs. MAE (Figure 20)"
      ],
      "metadata": {
        "id": "tyP4y6MpwZ8E"
      },
      "id": "tyP4y6MpwZ8E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot used in Figure 20"
      ],
      "metadata": {
        "id": "AUtC8S3RxYTG"
      },
      "id": "AUtC8S3RxYTG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range and the functions\n",
        "x = np.linspace(0, 1, 100)\n",
        "y1 = x**2\n",
        "y2 = x\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y1, label='MSE')\n",
        "plt.plot(x, y2, label='MAE')\n",
        "plt.xlabel('difference')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fek2MRMQwZdj"
      },
      "id": "Fek2MRMQwZdj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN starting with a permutation key with multiple fixed points (Figure 24)"
      ],
      "metadata": {
        "id": "tuLRBoOoz9dL"
      },
      "id": "tuLRBoOoz9dL"
    },
    {
      "cell_type": "markdown",
      "id": "Nep6_bMfSDtz",
      "metadata": {
        "id": "Nep6_bMfSDtz"
      },
      "source": [
        "#### Plot used in Figure 24"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how many fixed points should the keys have\n",
        "n_fixed_points = 15\n",
        "\n",
        "text = get_testing_texts(1)\n",
        "key = create_permutation_key(27, 27)\n",
        "\n",
        "# change the original key...\n",
        "intermediate_key = change_permutation_key(27 - n_fixed_points, key)\n",
        "ciphertext = substitute_encrypt(text, key)\n",
        "\n",
        "#... and pass it as a parameter\n",
        "likelihoods, true_percentages, _ = monte_carlo_algorithm(ciphertext, 3001, rnn_estimate_function, key=intermediate_key, original_text=text)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(likelihoods, label='RNN estimate', color='blue')\n",
        "plt.plot(true_percentages, label='% correct characters', color='orange')\n",
        "\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('percentage')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tYNQL5fT0Hbj"
      },
      "id": "tYNQL5fT0Hbj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "DBVLPoLnP0oX",
      "metadata": {
        "id": "DBVLPoLnP0oX"
      },
      "source": [
        "### Confusion matrices calculations (Table 6, 7, 8, 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data used for Table 6 and 7\n",
        "with a derangement key"
      ],
      "metadata": {
        "id": "CD_LRuDs4oQz"
      },
      "id": "CD_LRuDs4oQz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bf2jAiTa7nfe",
      "metadata": {
        "id": "Bf2jAiTa7nfe"
      },
      "outputs": [],
      "source": [
        "# getting 30 random texts\n",
        "plaintext_texts = get_testing_texts(30)\n",
        "#likelihood_func = rnn_estimate_function\n",
        "likelihood_func = markov_chain_analysis\n",
        "\n",
        "# data for the confusion matrix\n",
        "true_pos = 0\n",
        "true_neg = 0\n",
        "false_pos = 0\n",
        "false_neg = 0\n",
        "\n",
        "for plaintext_text in plaintext_texts:\n",
        "  for i in range(10):\n",
        "    # key to encrypt + create ciphertext\n",
        "    encrypt_key = create_permutation_key(27, 27)\n",
        "    ciphertext = substitute_encrypt(plaintext_text, encrypt_key)\n",
        "\n",
        "    # ensuring derangement with 0 fixed points as the key to encrypt\n",
        "    while True:\n",
        "      key = create_permutation_key(27, 27)\n",
        "      if not sum([key[i] == encrypt_key[i] for i in range(len(key))]):\n",
        "        key = np.array(key)\n",
        "        break\n",
        "\n",
        "    # decrypting using the new key\n",
        "    decrypted = substitute_decrypt(ciphertext, key)\n",
        "    # internal likelihood (should be 0)\n",
        "    original_plaus = likelihood_func(decrypted)\n",
        "\n",
        "    # trying all possible swaps\n",
        "    for index1 in range(27):\n",
        "      for index2 in range(index1 + 1, 27):\n",
        "        new_key = np.array(key)\n",
        "        new_key[index1], new_key[index2] = new_key[index2], new_key[index1]\n",
        "\n",
        "        correct_chars_before = compare_texts(decrypted, plaintext_text)\n",
        "        decrypted_text = substitute_decrypt(ciphertext, new_key)\n",
        "        new_correct_chars = compare_texts(decrypted_text, plaintext_text)\n",
        "        mc_plaus = likelihood_func(decrypted_text)\n",
        "        # true likelihood improved...\n",
        "        if new_correct_chars > correct_chars_before:\n",
        "          # ... and the model noticed --> TP\n",
        "          if mc_plaus > original_plaus:\n",
        "            true_pos += 1\n",
        "          # ... and the model did NOT notice --> FN\n",
        "          else:\n",
        "            false_neg += 1\n",
        "        # true likelihood did not improve...\n",
        "        else:\n",
        "          # ... and the model noticed --> TN\n",
        "          if mc_plaus < original_plaus:\n",
        "            true_neg += 1\n",
        "          # ... and the model did NOT notice --> FP\n",
        "          else:\n",
        "            false_pos += 1\n",
        "\n",
        "print(f'TP {true_pos}\\nFP {false_pos}\\nFN {false_neg}\\nTN {true_neg}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data used for Table 8 and 9\n",
        "with a key after 1000 iterations"
      ],
      "metadata": {
        "id": "XaDR9zrC7Bdg"
      },
      "id": "XaDR9zrC7Bdg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pVCYf8Zoaiz1",
      "metadata": {
        "id": "pVCYf8Zoaiz1"
      },
      "outputs": [],
      "source": [
        "# getting 30 random texts\n",
        "plaintext_texts = get_testing_texts(30)\n",
        "\n",
        "#likelihood_func = rnn_estimate_function\n",
        "likelihood_func = markov_chain_analysis\n",
        "\n",
        "# data for the confusion matrix\n",
        "true_pos = 0\n",
        "true_neg = 0\n",
        "false_pos = 0\n",
        "false_neg = 0\n",
        "\n",
        "for plaintext_text in plaintext_texts:\n",
        "  encrypt_key = create_permutation_key(27, 27)\n",
        "  ciphertext = substitute_encrypt(plaintext_text, encrypt_key)\n",
        "\n",
        "\n",
        "  for j in range(10):\n",
        "    decrypt_key = create_permutation_key(27, 27)\n",
        "    intermediate_key = monte_carlo_algorithm(ciphertext, 1001, likelihood_func, plaintext_text, return_key=True)\n",
        "    current_decrypted_text = substitute_decrypt(ciphertext, intermediate_key)\n",
        "    original_plaus = likelihood_func(current_decrypted_text)\n",
        "\n",
        "\n",
        "    for index1 in range(27):\n",
        "      for index2 in range(index1 + 1, 27):\n",
        "        new_key = np.array(intermediate_key)\n",
        "        new_key[index1], new_key[index2] = new_key[index2], new_key[index1]\n",
        "        correct_chars_before = sum([1 if current_decrypted_text[j] == plaintext_text[j] else 0 for j in range(512)])\n",
        "        new_decrypted_text = substitute_decrypt(ciphertext, new_key)\n",
        "        new_correct_chars = sum([1 if plaintext_text[j] == new_decrypted_text[j] else 0 for j in range(512)])\n",
        "\n",
        "        mc_plaus = likelihood_func(new_decrypted_text)\n",
        "\n",
        "        # true likelihood improved...\n",
        "        if new_correct_chars > correct_chars_before:\n",
        "          # ... and the model noticed --> TP\n",
        "          if mc_plaus > original_plaus:\n",
        "            true_pos += 1\n",
        "          # ... and the model did NOT notice --> FN\n",
        "          else:\n",
        "            false_neg += 1\n",
        "        # true likelihood did not improve...\n",
        "        else:\n",
        "          # ... and the model noticed --> TN\n",
        "          if mc_plaus < original_plaus:\n",
        "            true_neg += 1\n",
        "          # ... and the model did NOT notice --> FP\n",
        "          else:\n",
        "            false_pos += 1\n",
        "print(f'TP {true_pos}\\nFP {false_pos}\\nFN {false_neg}\\nTN {true_neg}\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kbXp3i8yiqWL",
      "metadata": {
        "id": "kbXp3i8yiqWL"
      },
      "source": [
        "### Sensitivity and Specificity in time (Figure 25, 26)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FzkDuFeFipb8",
      "metadata": {
        "id": "FzkDuFeFipb8"
      },
      "outputs": [],
      "source": [
        "# local search for a key to substitution cipher with checking several swaps each iterations\n",
        "def monte_carlo_algorithm_sens_spec(ciphertext, n_iterations, likelihood_func, original_text, return_key=False):\n",
        "  # array for storing algorithm data\n",
        "  confusion_matrix_data = [{'TN':0, 'TP':0, 'FN':0, 'FP':0} for j in range(n_iterations // 200)]\n",
        "  likelihoods = []\n",
        "  true_percentages = []\n",
        "  # creating a random key - vector of numbers 0 - 26\n",
        "  key = np.random.permutation(27)\n",
        "\n",
        "\n",
        "  # initial evaluation values\n",
        "  iter_decrypt = substitute_decrypt(ciphertext, key)\n",
        "  iter_likelihood = likelihood_func(iter_decrypt)\n",
        "  true_percentage = compare_texts(iter_decrypt, original_text)\n",
        "\n",
        "  # tracking made actions\n",
        "  swaps, rejected, randomwalk = 0,0,0\n",
        "\n",
        "  for i in range(n_iterations):\n",
        "      new_key = np.copy(key)\n",
        "\n",
        "      # after each iteration, simulation of 80 possible swaps is done\n",
        "      #_______________________________________________________________________________________________________\n",
        "\n",
        "      true_pos = 0\n",
        "      true_neg = 0\n",
        "      false_pos = 0\n",
        "      false_neg = 0\n",
        "\n",
        "      for swap in range(80):\n",
        "          x_index1, x_index2 = random.sample(range(27), 2)\n",
        "          x_original_plaus = iter_likelihood\n",
        "\n",
        "          x_correct_chars_before = sum([1 if iter_decrypt[j] == original_text[j] else 0 for j in range(512)])\n",
        "\n",
        "          x_new_key = np.array(key)\n",
        "          x_new_key[x_index1], x_new_key[x_index2] = x_new_key[x_index2], x_new_key[x_index1]\n",
        "\n",
        "          x_decrypted_text = substitute_decrypt(ciphertext, x_new_key)\n",
        "\n",
        "          x_new_correct_chars = sum([1 if original_text[j] == x_decrypted_text[j] else 0 for j in range(512)])\n",
        "\n",
        "          x_mc_plaus = likelihood_func(x_decrypted_text)\n",
        "\n",
        "          # true likelihood improved...\n",
        "          if x_new_correct_chars > x_correct_chars_before:\n",
        "            # ... and the model noticed --> TP\n",
        "            if x_mc_plaus > x_original_plaus:\n",
        "              true_pos += 1\n",
        "            # ... and the model did NOT notice --> FN\n",
        "            else:\n",
        "              false_neg += 1\n",
        "          # true likelihood did not improve...\n",
        "          else:\n",
        "            # ... and the model noticed --> TN\n",
        "            if x_mc_plaus < x_original_plaus:\n",
        "              true_neg += 1\n",
        "            # ... and the model did NOT notice --> FP\n",
        "            else:\n",
        "              false_pos += 1\n",
        "\n",
        "      confusion_matrix_data[i // 200]['TP'] += true_pos\n",
        "      confusion_matrix_data[i // 200]['TN'] += true_neg\n",
        "      confusion_matrix_data[i // 200]['FP'] += false_pos\n",
        "      confusion_matrix_data[i // 200]['FN'] += false_neg\n",
        "\n",
        "      #_______________________________________________________________________________________________________\n",
        "\n",
        "\n",
        "      # randomly swapping two elements of the key vector\n",
        "      index1, index2 = np.random.choice(len(new_key), size=2, replace=False)\n",
        "      new_key[index1], new_key[index2] = new_key[index2], new_key[index1]\n",
        "\n",
        "      # decrypting text with the new vector and computing its likelihood\n",
        "      new_iter_decrypt = substitute_decrypt(ciphertext, new_key)\n",
        "      new_iter_likelihood = likelihood_func(new_iter_decrypt)\n",
        "      new_true_percentage = compare_texts(new_iter_decrypt, original_text)\n",
        "\n",
        "      # computing ratio --> how much has the internal likelihood improved?\n",
        "      q = new_iter_likelihood / iter_likelihood\n",
        "\n",
        "      # boolean --> has the true likelihood actually improved?\n",
        "      true_q = new_true_percentage > true_percentage\n",
        "\n",
        "\n",
        "      # if the internal likelihood improved, swap is approved --> hill climb\n",
        "      if (q > 1):\n",
        "          key = new_key\n",
        "          iter_decrypt = new_iter_decrypt\n",
        "          iter_likelihood = new_iter_likelihood\n",
        "          true_percentage = new_true_percentage\n",
        "          swaps += 1\n",
        "\n",
        "\n",
        "      # if the internal likelihood has only slightly decreased + odds --> random walk\n",
        "      elif (q > 0.9 and np.random.rand() < 0.001):\n",
        "          key = new_key\n",
        "          iter_decrypt = new_iter_decrypt\n",
        "          iter_likelihood = new_iter_likelihood\n",
        "          true_percentage = new_true_percentage\n",
        "          randomwalk += 1\n",
        "\n",
        "      # if the internal likelihood hasn't improved --> reject\n",
        "      else:\n",
        "          rejected += 1\n",
        "\n",
        "      likelihoods.append(iter_likelihood)\n",
        "      true_percentages.append(true_percentage)\n",
        "\n",
        "      # console log\n",
        "      if (i % 1000 == 0):\n",
        "          print(f'{i}. iteration', iter_likelihood)\n",
        "  print(f'swaps/rejected/random {swaps}/{rejected}/{randomwalk}')\n",
        "  if return_key:\n",
        "    return key\n",
        "  return likelihoods, true_percentages, confusion_matrix_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_path = f'{my_path}/variables/saved_variables_rnn.pkl'\n",
        "data_path = f'{my_path}/variables/saved_variables_mc.pkl'\n",
        "\n",
        "# likelihood_function = rnn_estimate_function\n",
        "likelihood_function = markov_chain_analysis\n",
        "\n",
        "texts = get_testing_texts(30)\n",
        "function_data = []\n",
        "\n",
        "for text in texts:\n",
        "  datapoint = {'likelihoods':None, 'true_percentages':None, 'sensitivity':None, 'specificity':None}\n",
        "  key = np.random.permutation(27)\n",
        "  ciphertext = substitute_encrypt(text, key)\n",
        "  likelihoods, true_percentages, confusion_matrix_data = monte_carlo_algorithm_sens_spec(ciphertext, 2000, likelihood_function, text)\n",
        "  sensitivity = []\n",
        "  specificity = []\n",
        "\n",
        "  # calculation sensitivity and specificity\n",
        "  for i in confusion_matrix_data:\n",
        "    sensitivity.append(i['TP'] / (i['TP'] + i['FN']))\n",
        "    specificity.append(i['TN'] / (i['TN'] + i['FP']))\n",
        "\n",
        "    # scaling Markov Chains log_likelihoods\n",
        "    if likelihood_function == markov_chain_analysis:\n",
        "      maxi = markov_chain_analysis(text)\n",
        "      mini = min(likelihoods)\n",
        "      likelihoods = [(i - mini) / (maxi - mini)  for i in likelihoods]\n",
        "\n",
        "  datapoint['likelihoods'] = likelihoods\n",
        "  datapoint['true_percentages'] = true_percentages\n",
        "  datapoint['sensitivity'] = sensitivity\n",
        "  datapoint['specificity'] = specificity\n",
        "\n",
        "  # saving data each iteration\n",
        "  function_data.append(datapoint)\n",
        "  with open(data_path, 'wb') as file:\n",
        "    pickle.dump({\n",
        "        'mc_data': function_data},\n",
        "                file)\n",
        "\n",
        "  print('Done')\n"
      ],
      "metadata": {
        "id": "knh7PU_JpF1H"
      },
      "id": "knh7PU_JpF1H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_time_conf_data(dictionary, RNN=False):\n",
        "  log_likelihoods = dictionary['likelihoods']\n",
        "  true_correct_chars = dictionary['true_percentages']\n",
        "  sensitivity = dictionary['sensitivity']\n",
        "  specificity = dictionary['specificity']\n",
        "\n",
        "  # Creating x values for the lines\n",
        "  x_values = np.arange(2000)\n",
        "\n",
        "  # Creating x values for the bars\n",
        "  bar_intervals = [(i * 200, (i + 1) * 200) for i in range(10)]\n",
        "\n",
        "  fig, ax1 = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "  # Plotting log-likelihoods and true percentage of correct characters as lines\n",
        "\n",
        "  if RNN:\n",
        "    label_func = 'RNN estimate'\n",
        "  else:\n",
        "    label_func = 'scaled Log-Likelihood'\n",
        "\n",
        "  ax1.plot(x_values, log_likelihoods, label=label_func, color='blue')\n",
        "  ax1.plot(x_values, true_correct_chars, label='True % Correct Characters', color='green')\n",
        "  ax1.set_xlabel('iteration')\n",
        "  ax1.set_ylabel('percentage')\n",
        "  ax1.legend(loc='upper right')\n",
        "\n",
        "  # Plotting sensitivity and specificity as bars\n",
        "  for i, (start, end) in enumerate(bar_intervals):\n",
        "      ax1.bar(start + 50, sensitivity[i], width=100, color='red', alpha=0.5, label='Sensitivity' if i == 0 else '')\n",
        "      ax1.bar(start + 150, specificity[i], width=100, color='orange', alpha=0.5, label='Specificity' if i == 0 else '')\n",
        "\n",
        "  ax1.legend(loc='center right')\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "6QL8Mzhx0sib"
      },
      "id": "6QL8Mzhx0sib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The execution time is extreme, data can be loaded from drive"
      ],
      "metadata": {
        "id": "6WO-rucFVSa5"
      },
      "id": "6WO-rucFVSa5"
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "data_path = f'{my_path}/variables/saved_variables_rnn.pkl'\n",
        "with open(data_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "rnn_data = data['rnn_data']"
      ],
      "metadata": {
        "id": "UNT4uHVYFGLe"
      },
      "id": "UNT4uHVYFGLe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "data_path = f'{my_path}/variables/saved_variables_mc.pkl'\n",
        "with open(data_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "markov_chain_data = data['markov_chain_data']"
      ],
      "metadata": {
        "id": "u-bAVjlssOM6"
      },
      "id": "u-bAVjlssOM6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Computing averages"
      ],
      "metadata": {
        "id": "aXywWKj1Vc3y"
      },
      "id": "aXywWKj1Vc3y"
    },
    {
      "cell_type": "code",
      "source": [
        "markov_chain_avg = {'likelihoods':np.zeros(2000), 'true_percentages':np.zeros(2000), 'sensitivity':np.zeros(10), 'specificity':np.zeros(10)}\n",
        "n_data = len(markov_chain_data)\n",
        "for mc in markov_chain_data:\n",
        "  markov_chain_avg['likelihoods'] += np.array(mc['plausibilities']) / n_data\n",
        "  markov_chain_avg['true_percentages'] += np.array(mc['true_plausibilities']) / n_data\n",
        "  markov_chain_avg['sensitivity'] += np.array(mc['sensitivity']) / n_data\n",
        "  markov_chain_avg['specificity'] += np.array(mc['specificity']) / n_data"
      ],
      "metadata": {
        "id": "nm_I49eQqfAm"
      },
      "id": "nm_I49eQqfAm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_avg = {'likelihoods':np.zeros(2000), 'true_percentages':np.zeros(2000), 'sensitivity':np.zeros(10), 'specificity':np.zeros(10)}\n",
        "n_data = len(rnn_data)\n",
        "for rnn in rnn_data:\n",
        "  rnn_avg['likelihoods'] += np.array(rnn['plausibilities']) / n_data\n",
        "  rnn_avg['true_percentages'] += np.array(rnn['true_plausibilities']) / n_data\n",
        "  rnn_avg['sensitivity'] += np.array(rnn['sensitivity']) / n_data\n",
        "  rnn_avg['specificity'] += np.array(rnn['specificity']) / n_data"
      ],
      "metadata": {
        "id": "Y1V5vTa3zDVe"
      },
      "id": "Y1V5vTa3zDVe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot used in Figure 25 and 26"
      ],
      "metadata": {
        "id": "CYMpQGeeVhwa"
      },
      "id": "CYMpQGeeVhwa"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_time_conf_data(rnn_avg, RNN=True)"
      ],
      "metadata": {
        "id": "VkhMTaarp9Jt"
      },
      "id": "VkhMTaarp9Jt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_time_conf_data(markov_chain_avg)"
      ],
      "metadata": {
        "id": "eslrypvsuIsu"
      },
      "id": "eslrypvsuIsu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "O6Aa3zi8FlOA",
      "metadata": {
        "id": "O6Aa3zi8FlOA"
      },
      "source": [
        "### Confusion matrix decisions together with RNN estimates/log-likelihood (Figure 27, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AhETos3mRyOD",
      "metadata": {
        "id": "AhETos3mRyOD"
      },
      "source": [
        "#### Plot used in Figure 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exKQWyC6NChp",
      "metadata": {
        "id": "exKQWyC6NChp"
      },
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "  text = get_testing_texts(1)\n",
        "  key = create_permutation_key(27, 27)\n",
        "\n",
        "  plaintext_likelihood = markov_chain_analysis(text)\n",
        "\n",
        "  ciphertext = substitute_encrypt(text, key)\n",
        "  likelihoods, true_chars_percent, confus=monte_carlo_algorithm(ciphertext, 5001, markov_chain_analysis, original_text=text)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "\n",
        "  likelihoods = [(i - min(likelihoods)) / (plaintext_likelihood - min(likelihoods)) for i in likelihoods]\n",
        "\n",
        "  # plot the likelihood and correct characters % using the index as the x-value\n",
        "  plt.plot(likelihoods, label='scaled log-likelihood', color='blue')\n",
        "  plt.plot(true_chars_percent, label='percentage of correct characters', color='orange')\n",
        "\n",
        "  # generate x coordinates (indices of the array)\n",
        "  x_coords = range(len(confus))\n",
        "\n",
        "  # compute y coordinates (confus values divided by 3)\n",
        "  y_coords = [num / 3 for num in confus]\n",
        "\n",
        "  # define colors and labels for each value\n",
        "  colors = {0: 'purple', 1: 'red', 2: 'grey', 3: 'green'}\n",
        "  labels = {0: 'False Negaties', 1: 'False Positives', 2: 'True Negatives', 3: 'True Positives'}\n",
        "  plotted_labels = set()\n",
        "\n",
        "  # each point with a different color\n",
        "  for x, y in zip(x_coords, y_coords):\n",
        "      value = int(y * 3)\n",
        "      if value not in plotted_labels:\n",
        "          plt.scatter(x, y, color=colors[value], label=labels[value])\n",
        "          plotted_labels.add(value)\n",
        "      else:\n",
        "          plt.scatter(x, y, color=colors[value])\n",
        "\n",
        "  plt.xlabel('iteration')\n",
        "  plt.ylabel('percentage')\n",
        "  plt.legend()\n",
        "\n",
        "  # save the plot\n",
        "  plt.savefig(f'{my_path}/plots/markov_chain_test_conf{_}.png')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0kuOgjoiRppX",
      "metadata": {
        "id": "0kuOgjoiRppX"
      },
      "source": [
        "#### Plot used in Figure 28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7yz2aR4QZDp",
      "metadata": {
        "id": "e7yz2aR4QZDp"
      },
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "  text = get_testing_texts(1)\n",
        "  key = create_permutation_key(27, 27)\n",
        "\n",
        "  ciphertext = substitute_encrypt(text, key)\n",
        "  likelihoods, true_chars_percent, likelihoods_second_func, confus = monte_carlo_algorithm(ciphertext, 3001, rnn_estimate_function, original_text=text)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "\n",
        "  # plot the likelihood and correct characters % using the index as the x-value\n",
        "  plt.plot(likelihoods, label='RNN estimate', color='blue')\n",
        "  plt.plot(true_chars_percent, label='percentage of correct characters', color='orange')\n",
        "\n",
        "  # generate x coordinates (indices of the array)\n",
        "  x_coords = range(len(confus))\n",
        "\n",
        "  # compute y coordinates (confus values divided by 3)\n",
        "  y_coords = [num / 3 for num in confus]\n",
        "\n",
        "  # define colors and labels for each value\n",
        "  colors = {0: 'purple', 1: 'red', 2: 'grey', 3: 'green'}\n",
        "  labels = {0: 'False Negaties', 1: 'False Positives', 2: 'True Negatives', 3: 'True Positives'}\n",
        "  plotted_labels = set()\n",
        "\n",
        "  # plot each point with a different color\n",
        "  for x, y in zip(x_coords, y_coords):\n",
        "      value = int(y * 3)\n",
        "      if value not in plotted_labels:\n",
        "          plt.scatter(x, y, color=colors[value], label=labels[value])\n",
        "          plotted_labels.add(value)\n",
        "      else:\n",
        "          plt.scatter(x, y, color=colors[value])\n",
        "\n",
        "  plt.xlabel('iteration')\n",
        "  plt.ylabel('percentage')\n",
        "  plt.legend()\n",
        "\n",
        "  # save the plot\n",
        "  plt.savefig(f'{my_path}/plots/rnn_test_conf{_}.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f7c913f-7b15-4a1c-b619-9b4d48f7a42c",
      "metadata": {
        "id": "8f7c913f-7b15-4a1c-b619-9b4d48f7a42c"
      },
      "source": [
        "### Data pipelines\n",
        "downloading data in .csv format\n",
        "\n",
        "[Wikipedia Webscrapping]() the most topics, use of webscrapping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c474b7a-bbe2-49c5-86c6-8cf133ec13a0",
      "metadata": {
        "id": "3c474b7a-bbe2-49c5-86c6-8cf133ec13a0"
      },
      "outputs": [],
      "source": [
        "def fetch_random_wikipedia_articles(language='en', num_articles=50):\n",
        "    # Wikipedia API endpoint\n",
        "    url = f'https://{language}.wikipedia.org/w/api.php'\n",
        "\n",
        "    # Parameters for the API request to get random articles\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'format': 'json',\n",
        "        'list': 'random',  # random articles\n",
        "        'rnnamespace': 0,  # namespace 0 is for main articles (not user pages)\n",
        "        'rnlimit': num_articles  # number of random articles to return\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    articles_content = []\n",
        "\n",
        "    for article in data['query']['random']:\n",
        "        page_title = article['title']\n",
        "\n",
        "        # Parameters for the API request to get the content of the article\n",
        "        content_params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'extracts',\n",
        "            'explaintext': True,  # get the content as plaintext\n",
        "        }\n",
        "\n",
        "        content_response = requests.get(url, params=content_params)\n",
        "        content_data = content_response.json()\n",
        "\n",
        "        # Extract page id to access the content\n",
        "        page_id = next(iter(content_data['query']['pages']))\n",
        "        content = content_data['query']['pages'][page_id]['extract']\n",
        "\n",
        "        articles_content.append(content)\n",
        "\n",
        "    return ' '.join(articles_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3df753da-901b-4e47-a3e0-47b54b32e4ee",
      "metadata": {
        "id": "3df753da-901b-4e47-a3e0-47b54b32e4ee"
      },
      "outputs": [],
      "source": [
        "def split_string_fixed_length(input_string, length):\n",
        "    # Split the string into chunks of the specified length\n",
        "    # The remainder that doesn't fit into the length is cut off\n",
        "    chunks = [input_string[i:i+length] for i in range(0, len(input_string), length)]\n",
        "\n",
        "    if len(chunks[-1]) < length:\n",
        "        chunks = chunks[:-1]\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0300aef9-f8ed-4375-856d-8c0fe96e868d",
      "metadata": {
        "id": "0300aef9-f8ed-4375-856d-8c0fe96e868d"
      },
      "outputs": [],
      "source": [
        "def webscrapping_wiki(length=128, n_datapoints=250000, n_swapped_elements=0, language='en'):\n",
        "    filename = f'{my_path}/wikidata/512_data.csv'\n",
        "\n",
        "    with open(filename, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        counter = 0\n",
        "        while counter < n_datapoints:\n",
        "            random_text = fetch_random_wikipedia_articles(language, 100)  # random texts from Wikipedia\n",
        "            try:\n",
        "                formatted_random_text = format_text(random_text)  # formatting to A-Z & _\n",
        "            except AssertionError:  # sometimes empty string is fetched, then formatting fails\n",
        "                continue\n",
        "            else:\n",
        "                text_chunked = split_string_fixed_length(formatted_random_text, length)  # creating chunks\n",
        "                if len(text_chunked) + counter > n_datapoints:  # ensuring the desired length of the dataset is not exceeded\n",
        "                    text_chunked = text_chunked[:n_datapoints - counter]\n",
        "                for text in text_chunked:\n",
        "                    new_key = create_permutation_key(n_swapped_elements, 27)\n",
        "                    encrypted_text = substitute_encrypt(text, new_key)\n",
        "                    encoding = sum([1 if text[i] == encrypted_text[i] else 0 for i in range(len(text)) ]) / len(text)\n",
        "                    writer.writerow([encrypted_text, encoding])\n",
        "                counter += len(text_chunked)\n",
        "        print(f'encoding {counter} texts done')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7d4ca27-1bcd-4b01-b097-0c80cba9ce46",
      "metadata": {
        "id": "b7d4ca27-1bcd-4b01-b097-0c80cba9ce46"
      },
      "source": [
        "#### Webscrapping call\n",
        "scrapping data for 50000 texts of length 256 from wikipedia takes around 25 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e36acdcb-95e0-451f-b6ab-76ab009cf7ce",
      "metadata": {
        "id": "e36acdcb-95e0-451f-b6ab-76ab009cf7ce"
      },
      "outputs": [],
      "source": [
        "# need to be run only once\n",
        "# webscrapping_wiki(512, 38344, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f01a5c92-12e2-4ea8-9be7-57e6b5ccb465",
      "metadata": {
        "id": "f01a5c92-12e2-4ea8-9be7-57e6b5ccb465"
      },
      "outputs": [],
      "source": [
        "def combine_csv_files(file1, file2, output_file):\n",
        "    # Read the CSV files\n",
        "    df1 = pd.read_csv(file1)\n",
        "    df2 = pd.read_csv(file2)\n",
        "\n",
        "    # Concatenate the dataframes\n",
        "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "    # Write the combined dataframe to a new CSV file\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "\n",
        "# Example usage\n",
        "file1 = f'{my_path}/wikidata/new_128_data.csv'\n",
        "file2 = f'{my_path}/wikidata/128_250000_encrypted.csv'\n",
        "output_file = f'{my_path}/wikidata/model6.csv'\n",
        "combine_csv_files(file1, file2, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RdD2S1LYozx9",
      "metadata": {
        "id": "RdD2S1LYozx9"
      },
      "outputs": [],
      "source": [
        "# File path to the CSV file\n",
        "file_path = f'{my_path}/wikidata/512_data.csv'\n",
        "\n",
        "# Define column defaults: first column is a string, second column is a float\n",
        "column_defaults = [tf.string, tf.float32]\n",
        "\n",
        "# Create a CSV dataset\n",
        "dataset = tf.data.experimental.CsvDataset(\n",
        "    file_path,\n",
        "    record_defaults=column_defaults,\n",
        "    header=True,\n",
        ")\n",
        "\n",
        "# Convert the dataset to a list of tuples for iteration\n",
        "data_list = list(dataset.as_numpy_iterator())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7hETU4-mv99X",
      "metadata": {
        "id": "7hETU4-mv99X"
      },
      "outputs": [],
      "source": [
        "# Iterate through the dataset and perform manual changes\n",
        "altered_data = []\n",
        "for index, (text, encoding_level) in enumerate(data_list):\n",
        "    text = text.decode('utf-8')\n",
        "    new_key = create_permutation_key(index % 28, 27)\n",
        "    encrypted_text = substitute_encrypt(text, new_key)\n",
        "    encoding = sum([1 if text[i] == encrypted_text[i] else 0 for i in range(len(text)) ]) / len(text)\n",
        "\n",
        "    # Append the altered data to the list\n",
        "    altered_data.append((encrypted_text, encoding))\n",
        "\n",
        "# Convert the list of tuples to a DataFrame\n",
        "df = pd.DataFrame(altered_data, columns=['Text', 'EncodingLevel'])\n",
        "\n",
        "# Save the altered DataFrame to a new CSV file\n",
        "altered_file_path = f'{my_path}/wikidata/512_data_encoded.csv'\n",
        "df.to_csv(altered_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d4e2e39-21cd-4828-b8a1-e3e94d950376",
      "metadata": {
        "id": "0d4e2e39-21cd-4828-b8a1-e3e94d950376"
      },
      "source": [
        "### Line profiler\n",
        "checking the performance of the code - must be run within jupyter-notebook environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e0598c5-7e16-431a-a713-b39c7ad54494",
      "metadata": {
        "id": "9e0598c5-7e16-431a-a713-b39c7ad54494"
      },
      "outputs": [],
      "source": [
        "%load_ext line_profiler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de3557a2-03b6-400a-a3ee-c1990ac8544a",
      "metadata": {
        "id": "de3557a2-03b6-400a-a3ee-c1990ac8544a"
      },
      "source": [
        "#### format text function\n",
        "most expensive is the unidecode call - which is necessery to convert foreign characters into english ones\n",
        "\n",
        "checking, if the input text is already formatted using RegEx (also costly) is important to prevent sneaky bugs (HELLO_WORLD would turn into HELLOWORLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df86db31-41ac-4e67-aada-b25e5258d14a",
      "metadata": {
        "id": "df86db31-41ac-4e67-aada-b25e5258d14a"
      },
      "outputs": [],
      "source": [
        "%lprun -f format_text format_text(format_test_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebcabf3b-b461-4707-876d-eb239de0e4bc",
      "metadata": {
        "id": "ebcabf3b-b461-4707-876d-eb239de0e4bc"
      },
      "source": [
        "#### text to vector\n",
        "most expensive is formatting the text beforehand (unidecode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ef765c-0067-4a24-8b6b-ef84c9669f77",
      "metadata": {
        "id": "13ef765c-0067-4a24-8b6b-ef84c9669f77"
      },
      "outputs": [],
      "source": [
        "test_vector = text_to_vector(format_test_text)\n",
        "%lprun -f text_to_vector text_to_vector(format_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb991373-e9aa-4b38-9f9a-086f0bac8e08",
      "metadata": {
        "id": "cb991373-e9aa-4b38-9f9a-086f0bac8e08"
      },
      "outputs": [],
      "source": [
        "%lprun -f vector_to_text vector_to_text(test_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b71badee-6d24-49e2-96bf-b2d20d7c7cf2",
      "metadata": {
        "id": "b71badee-6d24-49e2-96bf-b2d20d7c7cf2"
      },
      "source": [
        "#### Substitute en-/decryption\n",
        "again - most costly is the subfunction text_to_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d1480b-45bc-498b-bdb3-e9a36b6d7fe6",
      "metadata": {
        "id": "e8d1480b-45bc-498b-bdb3-e9a36b6d7fe6"
      },
      "outputs": [],
      "source": [
        "test_key = np.random.permutation(27)\n",
        "%lprun -f substitute_encrypt substitute_encrypt(format_test_text, test_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7de8787-4fbe-4cb8-b8a7-be59a07eebe5",
      "metadata": {
        "id": "b7de8787-4fbe-4cb8-b8a7-be59a07eebe5"
      },
      "outputs": [],
      "source": [
        "%lprun -f substitute_decrypt substitute_decrypt(format_test_text, test_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd548b5-9b7f-4ed3-849b-868853019232",
      "metadata": {
        "id": "9dd548b5-9b7f-4ed3-849b-868853019232"
      },
      "source": [
        "#### Creating permutation key\n",
        "given percentage of swapped elements and number of elements in the vector\n",
        "most costly operation is using RNG to choose one of the remaining indices --> alternative approach would be to use the first index possible - but this would return the same permutation key everytime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b288bf-e503-49e3-b8ad-5f76603c00c1",
      "metadata": {
        "id": "57b288bf-e503-49e3-b8ad-5f76603c00c1"
      },
      "outputs": [],
      "source": [
        "%lprun -f create_permutation_key create_permutation_key(10, n_elements=27)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VlU8g1SI0c6f",
        "PxLmWtYHC_5L",
        "UMLrQXyryHef",
        "FvWoG_EbyLwR",
        "-ZiCriJrUctw",
        "2ffff869-d91f-4dcc-8690-4086f5bd605b",
        "35b9401a-ce04-492f-8830-66add2817384",
        "EUVlaUylb9v2",
        "oXjtjSL-Ll6s",
        "f9406e2b-33d6-4635-9b1b-e9043e8a9a9b",
        "IusDdvSLzob4",
        "OZNZ1GsX_6Iy",
        "VEB9HYhYT-_8",
        "fKZ03w4urT3R",
        "db7457eb-616a-400d-a8f8-4699111debd1",
        "ioflVlHhO99W",
        "qEyD_lfEP7Dp",
        "UfQgFHFyJgHI",
        "Tf0omxTFSiN2",
        "kfWnBUncvqUK",
        "9tsXwzONxsnS",
        "ue3gUishv1mX",
        "tyP4y6MpwZ8E",
        "AUtC8S3RxYTG",
        "tuLRBoOoz9dL",
        "Nep6_bMfSDtz",
        "DBVLPoLnP0oX",
        "CD_LRuDs4oQz",
        "XaDR9zrC7Bdg",
        "kbXp3i8yiqWL",
        "6WO-rucFVSa5",
        "aXywWKj1Vc3y",
        "CYMpQGeeVhwa",
        "O6Aa3zi8FlOA",
        "AhETos3mRyOD",
        "0kuOgjoiRppX",
        "8f7c913f-7b15-4a1c-b619-9b4d48f7a42c",
        "b7d4ca27-1bcd-4b01-b097-0c80cba9ce46",
        "0d4e2e39-21cd-4828-b8a1-e3e94d950376",
        "de3557a2-03b6-400a-a3ee-c1990ac8544a",
        "ebcabf3b-b461-4707-876d-eb239de0e4bc",
        "b71badee-6d24-49e2-96bf-b2d20d7c7cf2",
        "9dd548b5-9b7f-4ed3-849b-868853019232"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}