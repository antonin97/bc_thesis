{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonin97/bc_thesis/blob/main/training_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Colab environment"
      ],
      "metadata": {
        "id": "gsqaK5eV3ZOt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9xgYkslaOXo"
      },
      "outputs": [],
      "source": [
        "# mounting the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "my_path = '/content/drive/My Drive/bc_crypto'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local environment"
      ],
      "metadata": {
        "id": "Ub0z3OcWCCXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_path =  '.'"
      ],
      "metadata": {
        "id": "kXzTj_yiCC3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "6hZcOezL3d3w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0giLr_6Fabgw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfPPSvP5eEMd"
      },
      "source": [
        "### Hyperparameters\n",
        "currently set for the model_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1qP56ksdYqZ"
      },
      "outputs": [],
      "source": [
        "total_items = 1_000_000 # hardcoded\n",
        "text_length = 512\n",
        "\n",
        "validation_ratio = 0.05\n",
        "test_ratio = 0.05\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "embedding_dimension = 64\n",
        "learning_rate = 0.001\n",
        "epochs = 60\n",
        "\n",
        "ngram_size = 3\n",
        "vocab_percent = 0.99 # percent of all n-grams to be used for vocabulary (27**ngram_size * vacob_percent = vocab_size)\n",
        "\n",
        "identifier = '_'.join(str(datetime.datetime.now()).split()) # for unique identification of files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PL9OGfeJVc"
      },
      "source": [
        "### Data preprocessing pipeline\n",
        "Following dataset are needed for the NN training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUp8rUMbdWRV"
      },
      "outputs": [],
      "source": [
        "# creating a tf dataset from the csv file\n",
        "file_path = f'{my_path}/wikidata/512_data_encoded.csv'\n",
        "# first column = string, encrypted text | second  column = float, % of correctly placed characters <0, 1>\n",
        "column_defaults = [tf.string, tf.float32]\n",
        "\n",
        "# Create a CSV dataset\n",
        "dataset = tf.data.experimental.CsvDataset(\n",
        "    file_path,\n",
        "    record_defaults=column_defaults,\n",
        "    header=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G4PdvgweRY0"
      },
      "outputs": [],
      "source": [
        "test_size = int(total_items * test_ratio)\n",
        "validation_size = int(total_items * validation_ratio)\n",
        "train_size = total_items - test_size - validation_size\n",
        "\n",
        "full_dataset = dataset.shuffle(buffer_size=total_items) # to ensure uniform distribution, we need the buffer to consist of all the data\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_dataset = full_dataset.take(train_size)\n",
        "remaining_dataset = full_dataset.skip(train_size) # for further split to validation and test data; won't be used further\n",
        "validation_dataset = remaining_dataset.take(validation_size)\n",
        "test_dataset = remaining_dataset.skip(validation_size)\n",
        "\n",
        "# creating batches\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "validation_dataset = validation_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XKaeb9aeTAy"
      },
      "source": [
        "### Creating the NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCrnn6ggeU5E"
      },
      "source": [
        "#### Text vectorization\n",
        "vectorization layer will be adapted to the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyYuSDT7eTXF"
      },
      "outputs": [],
      "source": [
        "def ngram_split(text):\n",
        "    characters = tf.strings.unicode_split(text, 'UTF-8')\n",
        "    # Create character n-grams\n",
        "    return tf.strings.ngrams(\n",
        "        characters,\n",
        "        ngram_width=ngram_size, # global parameter\n",
        "        separator=''  # join n-grams without spaces\n",
        "\n",
        "    )\n",
        "\n",
        "# Create the TextVectorization layer\n",
        "text_vec = tf.keras.layers.TextVectorization(\n",
        "    standardize=None, # keeping underscores!\n",
        "    max_tokens=int((27**ngram_size)*vocab_percent), # computing vocabulary size on the fly\n",
        "    output_sequence_length=text_length - ngram_size + 1, # from the text of lenght 5, only 3 trigrams can be created (Equation 6)\n",
        "    split=ngram_split, # custom split function\n",
        "    output_mode='int',\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "# getting text data only from the training dataset\n",
        "text_data = train_dataset.map(lambda text, ident: text)\n",
        "\n",
        "\n",
        "# adapt the TextVectorization layer to the data\n",
        "text_vec.adapt(text_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the RNN\n",
        "only model_10 is included. All other models can be trained by changing hyperparameters according to Table 4"
      ],
      "metadata": {
        "id": "QwQlpY-Q4XRf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjheRvV5CGAo"
      },
      "source": [
        "#### Model 10 (Figure 12, 13)\n",
        "Two Bidirectional LSTM layers followed by a dense layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAlqU-lqCGAq"
      },
      "source": [
        "##### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_wOXDLZCGAq"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # Vectorization layer\n",
        "    text_vec,\n",
        "    # Embedding layer\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(text_vec.get_vocabulary()),\n",
        "        output_dim=embedding_dimension),\n",
        "    # hardcoded\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=False)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    # hardcoded\n",
        "])\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_error'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSN-hy7yCGAq"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Plot used in Figure 12"
      ],
      "metadata": {
        "id": "ez3SVnsgMN0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvBADMGaCGAr"
      },
      "outputs": [],
      "source": [
        "plot_model(model, show_shapes=True, show_layer_names=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXDMPggMCGAs"
      },
      "source": [
        "##### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29JGq-KKCGAs"
      },
      "outputs": [],
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor the validation loss\n",
        "    min_delta=0.00001,     # Minimum change to qualify as an improvement\n",
        "    patience=10,         # Number of epochs to wait for improvement\n",
        "    verbose=1,           # Print messages when stopping\n",
        "    mode='min',          # Stops training when the quantity monitored has stopped decreasing\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the minimum validation loss\n",
        ")\n",
        "\n",
        "\n",
        "checkpoints = keras.callbacks.ModelCheckpoint(\n",
        "    f'{my_path}/models/model_{identifier}_2BILSTM+Dense.tf',            # Path where the model will be saved\n",
        "    monitor='val_loss',         # Monitor validation loss to decide on saving\n",
        "    save_best_only=True,        # Save only when the validation loss improves\n",
        "    save_weights_only=False,    # Save the entire model, not just the weights\n",
        "    mode='min',                 # Save the model when the monitored metric decreases\n",
        "    verbose=1                   # Print out messages when saving the model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfS25UPeCGAt"
      },
      "source": [
        "##### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDSHMvmMCGAt"
      },
      "outputs": [],
      "source": [
        "# training took around 30 hours\n",
        "history = model.fit(\n",
        "     train_dataset,\n",
        "     epochs=epochs,\n",
        "     validation_data=validation_dataset,\n",
        "     callbacks=[early_stopping, checkpoints]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Plot used in Figure 13 (for the model_1)"
      ],
      "metadata": {
        "id": "9tRJl4-CManR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1oqB2DKCGAu"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(f'{my_path}/plots/loss_{identifier}_BILSTM+Dense.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hsjfTigCGAt"
      },
      "source": [
        "##### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clP5Do65CGAu"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset, verbose=1)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gsqaK5eV3ZOt",
        "Ub0z3OcWCCXr",
        "6hZcOezL3d3w",
        "NfPPSvP5eEMd",
        "b2PL9OGfeJVc",
        "1XKaeb9aeTAy",
        "pCrnn6ggeU5E",
        "QwQlpY-Q4XRf",
        "TjheRvV5CGAo",
        "WAlqU-lqCGAq",
        "ez3SVnsgMN0J",
        "tXDMPggMCGAs",
        "OfS25UPeCGAt",
        "9tRJl4-CManR",
        "8hsjfTigCGAt"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}